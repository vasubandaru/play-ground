{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forward Multiplicative Gate\n",
    "\n",
    "def forMulGate(x,y):\n",
    "    return x*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.950100000000001"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forMulGate(-1.99,2.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.992285139476791\n",
      "2.991595251530799\n"
     ]
    }
   ],
   "source": [
    "#Searching randomly for the values of x and y that will improve ciruit output\n",
    "import random\n",
    "x = -2\n",
    "y = 3\n",
    "tweak_amount = 0.01\n",
    "best_x = x\n",
    "best_y = y\n",
    "best_out = -float('inf')\n",
    "\n",
    "for i in range(100):\n",
    "    x_try = x + tweak_amount * (random.random() * 2 - 1)\n",
    "    y_try = y + tweak_amount * (random.random() * 2 - 1)\n",
    "    \n",
    "    out = forMulGate(x_try,y_try)\n",
    "#     print(x_try,y_try,out)\n",
    "    if out > best_out:\n",
    "        best_out = out\n",
    "        best_x = x_try\n",
    "        best_y = y_try\n",
    "\n",
    "print(best_x)\n",
    "print(best_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.000000000064062 -2.0000000000131024\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Searching using Gradient descent\n",
    "\"\"\"\n",
    "###### A simple derivative\n",
    "x=-2;y=3\n",
    "out = forMulGate(x,y)\n",
    "h= 0.00001\n",
    "\n",
    "# Derivative w.r.t x\n",
    "xph= x + h\n",
    "out1 = forMulGate(xph,y)\n",
    "x_derivative = (out1 - out)/h\n",
    "\n",
    "# Derivative w.r.t y\n",
    "yph= y + h\n",
    "out2 = forMulGate(x,yph)\n",
    "y_derivative = (out2 - out)/h\n",
    "\n",
    "print(x_derivative,y_derivative)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.870599999997832"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_size = 0.01\n",
    "\n",
    "x = x + step_size * x_derivative\n",
    "y = y + step_size * y_derivative\n",
    "forMulGate(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.870599999997832"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding analytic Gradient\n",
    "\n",
    "x = -2; y = 3\n",
    "out = forMulGate(x,y)\n",
    "\n",
    "x_gradient = y\n",
    "y_gradient = x\n",
    "\n",
    "step_size = 0.01\n",
    "\n",
    "x = x + step_size * x_derivative\n",
    "y = y + step_size * y_derivative\n",
    "forMulGate(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INPUT: We are given a circuit, some inputs and compute an output value.\n",
    "OUTPUT: We are then interested finding small changes to each input (independently) that would make the output higher.\n",
    "Strategy #1: One silly way is to randomly search for small pertubations of the inputs and keep track of what gives the highest increase in output.\n",
    "Strategy #2: We saw we can do much better by computing the gradient. Regardless of how complicated the circuit is, the numerical gradient is very simple (but relatively expensive) to compute. We compute it by probing the circuit’s output value as we tweak the inputs one at a time.\n",
    "Strategy #3: In the end, we saw that we can be even more clever and analytically derive a direct expression to get the analytic gradient. It is identical to the numerical gradient, it is fastest by far, and there is no need for any tweaking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4 -4 3\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Chain Rule (Example of : f(x,y,z)=(x+y)z) \"\"\"\n",
    "\n",
    "def forMulGate(x,y):\n",
    "    return x*y\n",
    "\n",
    "def forAddGate(x,y):\n",
    "    return x+y\n",
    "\n",
    "def forwardCircuit(x,y,z):\n",
    "    q = forAddGate(x,y)\n",
    "    f = forMulGate(q,z)\n",
    "    return f\n",
    "\n",
    "x = - 2; y = 5; z = -4\n",
    "forwardCircuit(x,y,z)\n",
    "\n",
    "\n",
    "### Gradient computation\n",
    "\n",
    "#Inital Conditions\n",
    "x = - 2; y = 5; z = -4\n",
    "q = forAddGate(x,y)\n",
    "f = forMulGate(q,z)\n",
    "\n",
    "#MUL Gate\n",
    "derivative_f_wrt_z = q\n",
    "derivative_f_wrt_q = z\n",
    "\n",
    "#ADD gate\n",
    "derivative_q_wrt_x = 1\n",
    "derivative_q_wrt_y = 1\n",
    "\n",
    "\n",
    "#Cahin rule\n",
    "derivative_f_wrt_x = derivative_f_wrt_q * derivative_q_wrt_x\n",
    "derivative_f_wrt_y = derivative_f_wrt_q * derivative_q_wrt_y\n",
    "\n",
    "print(derivative_f_wrt_x,derivative_f_wrt_y,derivative_f_wrt_z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.04 4.96 -3.97\n",
      "2.92 -11.5924\n"
     ]
    }
   ],
   "source": [
    "#Updated x,y,z\n",
    "\n",
    "x = - 2; y = 5; z = -4\n",
    "step_size = 0.01\n",
    "\n",
    "x = x + step_size * derivative_f_wrt_x\n",
    "y = y + step_size * derivative_f_wrt_y\n",
    "z = z + step_size * derivative_f_wrt_z\n",
    "print(x,y,z)\n",
    "\n",
    "q = forAddGate(x,y)\n",
    "f = forMulGate(q,z)\n",
    "\n",
    "print(q,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets recap once again what we learned:\n",
    "\n",
    "In the previous chapter we saw that in the case of a single gate (or a single expression), we can derive the analytic gradient using simple calculus. We interpreted the gradient as a force, or a tug on the inputs that pulls them in a direction which would make this gate’s output higher.\n",
    "\n",
    "In case of multiple gates everything stays pretty much the same way: every gate is hanging out by itself completely unaware of the circuit it is embedded in. Some inputs come in and the gate computes its output and the derivate with respect to the inputs. The only difference now is that suddenly, something can pull on this gate from above. That’s the gradient of the final circuit output value with respect to the ouput this gate computed. It is the circuit asking the gate to output higher or lower numbers, and with some force. The gate simply takes this force and multiplies it to all the forces it computed for its inputs before (chain rule). This has the desired effect:\n",
    "\n",
    "If a gate experiences a strong positive pull from above, it will also pull harder on its own inputs, scaled by the force it is experiencing from above\n",
    "And if it experiences a negative tug, this means that circuit wants its value to decrease not increase, so it will flip the force of the pull on its inputs to make its own output value smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.9999999999906777 -3.9999999999906777 3.000000000010772\n"
     ]
    }
   ],
   "source": [
    "#Sanity check using Numerical gradient\n",
    "\n",
    "x = -2; y = 5; z = -4\n",
    "\n",
    "h = 0.0001\n",
    "x_derivative = (forwardCircuit(x+h,y,z) - forwardCircuit(x,y,z))/ h\n",
    "y_derivative = (forwardCircuit(x,y+h,z) - forwardCircuit(x,y,z))/ h\n",
    "z_derivative = (forwardCircuit(x,y,z+h) - forwardCircuit(x,y,z))/ h\n",
    "\n",
    "print(x_derivative,y_derivative,z_derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Example: Single Neuron\n",
    "f(x,y,a,b,c)=σ(ax+by+c)\n",
    "\"\"\"\n",
    "\n",
    "class Unit():\n",
    "    \n",
    "    def __init__(self,value,grad):\n",
    "        self.value = value\n",
    "        self.grad = grad\n",
    "        print(self.value)\n",
    "\n",
    "class multiplyGate():\n",
    "    \n",
    "    def forward(self,u0,u1):\n",
    "        self.u0 = u0\n",
    "        self.u1 = u1\n",
    "        self.utop = Unit(u0.value * u1.value, 0.0)\n",
    "        return self.utop\n",
    "    \n",
    "    def backward(self):\n",
    "        self.u0.grad = self.u0.grad + (self.u1.value * self.utop.grad) \n",
    "        self.u1.grad = self.u1.grad + (self.u0.value * self.utop.grad) \n",
    "    \n",
    "class addGate():\n",
    "    \n",
    "    def forward(self,u0,u1):\n",
    "        self.u0 = u0\n",
    "        self.u1 = u1\n",
    "        self.utop = Unit(u0.value + u1.value, 0.0)\n",
    "        return self.utop\n",
    "    \n",
    "    def backward(self):\n",
    "        self.u0.grad = self.u0.grad + (self.utop.grad) \n",
    "        self.u1.grad = self.u1.grad + (self.utop.grad) \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class sigmoidGate():\n",
    "    \n",
    "    def sig(self,x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def forward(self,u0):\n",
    "        self.u0 = u0\n",
    "        self.utop = Unit(self.sig(self.u0.value), 0.0)\n",
    "        return self.utop\n",
    "    \n",
    "    def backward(self):\n",
    "        s = self.sig(self.u0.value)\n",
    "        self.u0.grad = self.u0.grad + (s * (1 - s)) * self.utop.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "2.0\n",
      "-3.0\n",
      "-1.0\n",
      "3.0\n",
      "-1.0\n",
      "6.0\n",
      "5.0\n",
      "2.0\n",
      "0.8807970779778823\n",
      "0.8807970779778823\n"
     ]
    }
   ],
   "source": [
    "# Create input units\n",
    "a = Unit(1.0, 0.0)\n",
    "b = Unit(2.0, 0.0)\n",
    "c = Unit(-3.0, 0.0)\n",
    "x = Unit(-1.0, 0.0)\n",
    "y = Unit(3.0, 0.0)\n",
    "\n",
    "# Create the gates\n",
    "mulg0 = multiplyGate()\n",
    "mulg1 = multiplyGate()\n",
    "addg0 = addGate()\n",
    "addg1 = addGate()\n",
    "sg0 = sigmoidGate()\n",
    "\n",
    "\n",
    "def forwardNeuron():\n",
    "    ax = mulg0.forward(a,x)\n",
    "    by = mulg1.forward(b,y)\n",
    "    axpby = addg0.forward(ax,by)\n",
    "    axpbypc = addg1.forward(axpby,c)\n",
    "    s = sg0.forward(axpbypc)\n",
    "    return s\n",
    "s = forwardNeuron()\n",
    "\n",
    "print(s.value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.grad = 1.0\n",
    "sg0.backward()\n",
    "addg1.backward()\n",
    "addg0.backward()\n",
    "mulg1.backward()\n",
    "mulg0.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.10499358540350662 0.31498075621051985 0.10499358540350662 0.10499358540350662 0.20998717080701323\n"
     ]
    }
   ],
   "source": [
    "print(a.grad,b.grad,c.grad,x.grad,y.grad)\n",
    "\n",
    "step_size = 0.01\n",
    "a.value += step_size * a.grad\n",
    "b.value += step_size * b.grad\n",
    "c.value += step_size * c.grad\n",
    "x.value += step_size * x.grad\n",
    "y.value += step_size * y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9979012306572276\n",
      "6.013655780294242\n",
      "5.015754549637014\n",
      "2.0168044854910496\n",
      "0.8825501816218984\n",
      "0.8825501816218984\n"
     ]
    }
   ],
   "source": [
    "s = forwardNeuron()\n",
    "print(s.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.10499758359205913 0.3149447748351797 0.10498958734506125 0.10498958734506125 0.2099711788272618\n"
     ]
    }
   ],
   "source": [
    "def forwardCircuitFast(a,b,c,x,y):\n",
    "    return 1/(1+np.exp(- (a*x+b*y+c)))\n",
    "\n",
    "a = 1;b = 2; c = -3; x = -1; y = 3\n",
    "h = 0.0001\n",
    "a_grad = (forwardCircuitFast(a+h,b,c,x,y) - forwardCircuitFast(a,b,c,x,y))/h\n",
    "b_grad = (forwardCircuitFast(a,b+h,c,x,y) - forwardCircuitFast(a,b,c,x,y))/h\n",
    "c_grad = (forwardCircuitFast(a,b,c+h,x,y) - forwardCircuitFast(a,b,c,x,y))/h\n",
    "x_grad = (forwardCircuitFast(a,b,c,x+h,y) - forwardCircuitFast(a,b,c,x,y))/h\n",
    "y_grad = (forwardCircuitFast(a,b,c,x,y+h) - forwardCircuitFast(a,b,c,x,y))/h\n",
    "\n",
    "print(a_grad,b_grad,c_grad,x_grad,y_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Becoming a Backprop Ninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Ex-1 Multiplication Gate \"\"\"\n",
    "x = a*b\n",
    "\n",
    "#Backward pass\n",
    "da = b * dx # Gradient of a\n",
    "db = a * dx # Gradient of b\n",
    "\n",
    "\n",
    "\"\"\" Ex-2 Addition Gate \"\"\"\n",
    "x = a + b\n",
    "\n",
    "#Backward pass\n",
    "da = 1.0 * dx # Gradient of a\n",
    "db = 1.0 * dx # Gradient of b\n",
    "\n",
    "\"\"\" Ex-3 x = a + b + c \"\"\"\n",
    "q = a + b\n",
    "x = q + c\n",
    "\n",
    "#Backward pass\n",
    "dc = 1.0 * dx\n",
    "dq = 1.0 * dx\n",
    "da = 1.0 * dq  #(or) da = 1.0 * dx\n",
    "db = 1.0 * dq  #(or) db = 1.0 * dx\n",
    "\n",
    "\n",
    "\"\"\" Ex-4 x = a * b + c (comining gates)\"\"\"\n",
    "x = a * b + c\n",
    "\n",
    "da = db * dx\n",
    "db = da * dx\n",
    "dc = 1.0 * dx\n",
    "\n",
    "\"\"\" Ex-5 Neuron \"\"\"\n",
    "q = a * x + b * y + c\n",
    "f = sig(q) # Sigmoid function\n",
    "\n",
    "df = 1\n",
    "dq = (f * (1-f)) * df\n",
    "\n",
    "\n",
    "da = x * dq\n",
    "db = y * dq\n",
    "dx = a * dq\n",
    "dy = b * dq\n",
    "dc = 1.0 * dq\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Ex-6 x = a*a \"\"\"\n",
    "x = a*a\n",
    "\n",
    "da = 2 * a * dx\n",
    "\n",
    "\"\"\" Ex-7 x= a*a + b*b + c*c \"\"\"\n",
    "x= a*a + b*b + c*c\n",
    "\n",
    "da = 2*a*dx\n",
    "db = 2*b*dx\n",
    "dc = 2*c*dx\n",
    "\n",
    "\"\"\" Ex-8 x= ((ab+c)d)^2 \"\"\"\n",
    "\n",
    "x1 = a*b + c\n",
    "x2 = x1 * d\n",
    "x = x2 * x2\n",
    "\n",
    "dx2 = 2 * x2 * dx\n",
    "dd = x1 * dx2\n",
    "dx1 = d * dx2\n",
    "da = b * dx1\n",
    "db = a * dx1\n",
    "dc = 1.0 * dx1\n",
    "\n",
    "\n",
    "\"\"\" Ex-8 division \"\"\"\n",
    "x = 1.0/a\n",
    "\n",
    "da = -(1.0)/(a*a)\n",
    "\n",
    "\n",
    "\"\"\" Ex-8 divison 2 \"\"\"\n",
    "x= (a+b)/(c+d)\n",
    "\n",
    "x1 = (a+b)\n",
    "x2 = (c+d)\n",
    "x3 = 1.0/x2\n",
    "x = x1*x3\n",
    "\n",
    "\n",
    "dx1 = x3 * dx\n",
    "dx3 = x1 * dx\n",
    "dx2 = (-(1.0)/(x2 * x2)) * dx3\n",
    "da = 1.0 * dx1\n",
    "db = 1.0 * dx1\n",
    "dc = 1.0 * dx2\n",
    "dd = 1.0 * dx2\n",
    "\n",
    "\"\"\" Ex-8 max \"\"\"\n",
    "x = max(a,b)\n",
    "\n",
    "da = 1.0 * dx if a == x else 0.0\n",
    "db = 1.0 * dx if b == x else 0.0\n",
    "\n",
    "\"\"\" Ex-8 ReLU \"\"\"\n",
    "x = max(a,0)\n",
    "\n",
    "da = 1.0 * dx if a>0 else 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything we’ve done in this chapter comes down to this: We saw that we can feed some input through arbitrarily complex real-valued circuit, tug at the end of the circuit with some force, and backpropagation distributes that tug through the entire circuit all the way back to the inputs. If the inputs respond slightly along the final direction of their tug, the circuit will “give” a bit along the original pull direction. Maybe this is not immediately obvious, but this machinery is a powerful hammer for Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Circuit():\n",
    "    \n",
    "    self.mulg0 = multiplyGate()\n",
    "    self.mulg1 = multiplyGate()\n",
    "    self.addg0 = addGate()\n",
    "    self.addg1 = addGate()\n",
    "    \n",
    "    def forward(self,x,y,a,b,c):\n",
    "        self.ax = self.mulg0.forward(a,x)\n",
    "        self.by = self.mulg1.forward(b,y)\n",
    "        self.axpby = self.addg0.forward(self.ax,self.by)\n",
    "        self.axpbypc = self.addg1.forward(self.axpby,c)\n",
    "        \n",
    "        return self.axpbypc\n",
    "    \n",
    "    def backward(gradient_top):\n",
    "        self.axpbyc.grad = gradient_top\n",
    "        self.addg1.backward()\n",
    "        self.addg0.backward()\n",
    "        self.mulg1.backward()\n",
    "        self.mulg0.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM class\n",
    "\n",
    "class SVM():\n",
    "    \n",
    "    self.a = Unit(1.0,0.0)\n",
    "    self.b = Unit(-2.0,0.0)\n",
    "    self.c = Unit(-1.0,0.0)\n",
    "    \n",
    "    self.circuit = Circuit()\n",
    "    \n",
    "    def forward(self,x,y):\n",
    "        self.unit_out = self.circuit.forward(x,y,self.a,self.b,self.c)\n",
    "        return self.unit_out\n",
    "    \n",
    "    def backward(label):\n",
    "        \n",
    "        self.a.grad = 0\n",
    "        self.b.grad = 0\n",
    "        self.c.grad = 0\n",
    "        \n",
    "        pull = 0.0\n",
    "        if(label == 1 && self.unit_out.value < 1):\n",
    "            pull = 1\n",
    "        if(label == -1 && self.unit_out.value > -1):\n",
    "            pull = -1\n",
    "            \n",
    "        self.circuit.backward(pull)\n",
    "        \n",
    "        self.a.grad += - self.a.value\n",
    "        self.b.grad += - self.b.value\n",
    "        \n",
    "        def learnFrom(x,y,label):\n",
    "            self.forward(x,y)\n",
    "            self.backward(label)\n",
    "            self.parameterUpdate()\n",
    "            \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
